# run_predictions.py

import os
import json
from datetime import date, timedelta
import requests
import pandas as pd
import numpy as np
import yfinance as yf
from statsmodels.tsa.arima.model import ARIMA
import google.generativeai as genai
from collections import Counter

# --- AI ì„¤ì • ---
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    ai_model = genai.GenerativeModel('gemini-2.0-flash-lite')
else:
    ai_model = None
    print("âš ï¸  Gemini API í‚¤ê°€ ì—†ì–´ AI ë¶„ì„ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")

# --- ëª¨ë“  ê³„ì‚° í•¨ìˆ˜ë“¤ ---

def get_marketaux_news(api_key):
    if not api_key: return []
    url = f"https://api.marketaux.com/v1/news/all?countries=us&filter_entities=true&language=en&limit=10&api_token={api_key}"
    try:
        response = requests.get(url, timeout=15)
        response.raise_for_status()
        print("âœ… Marketaux ë‰´ìŠ¤ ìˆ˜ì§‘ ì„±ê³µ")
        return response.json().get('data', [])
    except requests.exceptions.RequestException as e:
        print(f"âŒ Marketaux API ì˜¤ë¥˜: {e}")
        return []

def analyze_article_with_ai(content):
    if not ai_model or not content or len(content) < 50: return {"summary": "ë¶„ì„ ë¶ˆê°€", "sentiment": 0.0, "keywords": []}
    prompt = f"""
ë‹¤ìŒ ê¸ˆìœµ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ ì •í™•íˆ ì´ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:
SENTIMENT: [â€“1.0ê³¼ 1.0 ì‚¬ì´ì˜ ìˆ«ìë§Œ]
SUMMARY: [í•œêµ­ì–´ë¡œ 3ë¬¸ì¥ ìš”ì•½]
KEYWORDS: [í‚¤ì›Œë“œ1], [í‚¤ì›Œë“œ2], [í‚¤ì›Œë“œ3]
ë‰´ìŠ¤ ë‚´ìš©:\n{content[:1000]}"""
    try:
        safety_settings = [{"category": c, "threshold": "BLOCK_NONE"} for c in ["HARM_CATEGORY_HARASSMENT", "HARM_CATEGORY_HATE_SPEECH", "HARM_CATEGORY_SEXUALLY_EXPLICIT", "HARM_CATEGORY_DANGEROUS_CONTENT"]]
        response = ai_model.generate_content(prompt, safety_settings=safety_settings, generation_config={"temperature": 0.3})
        text = response.text.strip()
        sentiment, summary, keywords = 0.0, "ë¶„ì„ ì‹¤íŒ¨", []
        for line in text.split('\n'):
            if line.startswith('SENTIMENT:'): sentiment = float(line.split(':', 1)[1].strip())
            elif line.startswith('SUMMARY:'): summary = line.split(':', 1)[1].strip()
            elif line.startswith('KEYWORDS:'): keywords = [k.strip() for k in line.split(':', 1)[1].split(',')]
        return {"summary": summary, "sentiment": max(-1.0, min(1.0, sentiment)), "keywords": keywords[:3]}
    except Exception as e:
        print(f"âŒ AI ë¶„ì„ ì˜¤ë¥˜: {e}")
        return {"summary": "ê¸°ì‚¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ", "sentiment": 0.0, "keywords": ["ì˜¤ë¥˜"]}

def generate_trend_summary_with_ai(keywords, sentiment_score):
    if not ai_model or not keywords: return {"title": "ë¶„ì„ ë¶ˆê°€", "summary": "ë°ì´í„° ë¶€ì¡±", "keywords": []}
    common_keywords = [item[0] for item in Counter(keywords).most_common(5)]
    prompt = f"""
ë‹¹ì‹ ì€ ì „ë¬¸ ê¸ˆìœµ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜¤ëŠ˜ ì‹œì¥ì˜ í•µì‹¬ ë™í–¥ì„ ë¶„ì„í•˜ê³ , íˆ¬ììë¥¼ ìœ„í•œ ê°„ê²°í•œ ìš”ì•½ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
- ì£¼ìš” í‚¤ì›Œë“œ: {', '.join(common_keywords)}
- ì¢…í•© ì‹œì¥ ê°ì„± ì§€ìˆ˜: {sentiment_score:.3f}
ì•„ë˜ í˜•ì‹ì„ ë°˜ë“œì‹œ ì§€ì¼œì„œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
TITLE: [ì˜¤ëŠ˜ì˜ ì‹œì¥ ë™í–¥ì„ ìš”ì•½í•˜ëŠ” ë§¤ë ¥ì ì¸ í•œ ë¬¸ì¥ ì œëª©]
SUMMARY: [ìœ„ ë°ì´í„°ë¥¼ ì¢…í•©í•˜ì—¬, ì „ë¬¸ê°€ì˜ ì‹œê°ìœ¼ë¡œ ì‹œì¥ ìƒí™©ì„ 2~3ë¬¸ì¥ìœ¼ë¡œ ë¶„ì„. ë¶€ë“œëŸ¬ìš´ ì¡°ì–¸ì˜ í†¤ìœ¼ë¡œ.]
"""
    try:
        response = ai_model.generate_content(prompt)
        text = response.text.strip()
        title, summary = "AI ë™í–¥ ë¶„ì„", "ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨"
        for line in text.split('\n'):
            if line.startswith('TITLE:'): title = line.split(':', 1)[1].strip()
            elif line.startswith('SUMMARY:'): summary = line.split(':', 1)[1].strip()
        return {"title": title, "summary": summary, "keywords": common_keywords}
    except Exception as e:
        print(f"âŒ AI íŠ¸ë Œë“œ ìš”ì•½ ì˜¤ë¥˜: {e}")
        return {"title": "AI ë™í–¥ ë¶„ì„ ì‹¤íŒ¨", "summary": "ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ", "keywords": common_keywords}

def get_chart_data(ticker, days=90, forecast_days=7):
    try:
        today = date.today()
        start_date = today - timedelta(days=days)
        data = yf.download(ticker, start=start_date, end=today, progress=False, timeout=15)
        if data.empty or len(data) < 20: return None
        hist_data = data['Close']
        model = ARIMA(hist_data, order=(5,1,0)).fit()
        forecast = model.forecast(steps=forecast_days)
        labels = [d.strftime('%m-%d') for d in hist_data.index] + [(today + timedelta(days=i)).strftime('%m-%d') for i in range(1, forecast_days + 1)]
        return {"labels": labels, "historical": np.round(hist_data.values, 2).tolist(), "forecast": [None] * len(hist_data) + np.round(forecast.values, 2).tolist()}
    except Exception as e:
        print(f"âŒ '{ticker}' ì°¨íŠ¸ ë°ì´í„° ìƒì„± ì˜¤ë¥˜: {e}")
        return None

def get_fx_chart_data(api_key, days=90, forecast_days=7):
    try:
        latest_rate = 1422.0
        if api_key:
            url = f"https://v6.exchangerate-api.com/v6/{api_key}/latest/USD"
            response = requests.get(url, timeout=10)
            if response.status_code == 200:
                latest_rate = response.json().get('conversion_rates', {}).get('KRW', latest_rate)
        today = date.today()
        start_date = today - timedelta(days=days)
        simulated_past = latest_rate + np.random.randn(days).cumsum()[::-1]
        hist_data = pd.Series(simulated_past, index=pd.to_datetime([start_date + timedelta(days=i) for i in range(days)]))
        model = ARIMA(hist_data, order=(5,1,0)).fit()
        forecast = model.forecast(steps=forecast_days)
        labels = [d.strftime('%m-%d') for d in hist_data.index] + [(today + timedelta(days=i)).strftime('%m-%d') for i in range(1, forecast_days + 1)]
        return {"labels": labels, "historical": np.round(hist_data.values, 2).tolist(), "forecast": [None] * len(hist_data) + np.round(forecast.values, 2).tolist()}
    except Exception as e:
        print(f"âŒ í™˜ìœ¨ ì°¨íŠ¸ ë°ì´í„° ìƒì„± ì˜¤ë¥˜: {e}")
        return None

if __name__ == "__main__":
    if not os.path.exists('data'):
        os.makedirs('data')

    print("--- ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘ ë° AI ë¶„ì„ ì‹œì‘ ---")
    marketaux_key = os.getenv("MARKETAUX_API_KEY")
    articles = get_marketaux_news(marketaux_key)
    processed_articles, all_keywords, total_sentiment = [], [], 0
    if articles:
        for article in articles:
            desc = article.get('description', '')
            if desc and len(desc) > 100:
                ai_result = analyze_article_with_ai(desc)
                article.update(ai_result)
                processed_articles.append(article)
                total_sentiment += ai_result.get('sentiment', 0.0)
                if ai_result.get('keywords'):
                    all_keywords.extend(ai_result['keywords'])
    market_sentiment_score = total_sentiment / len(processed_articles) if processed_articles else 0.0
    trend_summary = generate_trend_summary_with_ai(all_keywords, market_sentiment_score)
    print("âœ… ë‰´ìŠ¤ ë¶„ì„ ì™„ë£Œ.")

    print("\n--- ğŸ“ˆ ê·¸ë˜í”„ ë°ì´í„° ì˜ˆì¸¡ ì‹œì‘ ---")
    nasdaq_data = get_chart_data('^IXIC')
    kospi_data = get_chart_data('^KS11')
    fx_api_key = os.getenv("EXCHANGERATE_API_KEY")
    fx_data = get_fx_chart_data(fx_api_key)
    print("âœ… ê·¸ë˜í”„ ì˜ˆì¸¡ ì™„ë£Œ.")

    final_data = {
        "articles": processed_articles,
        "trend_summary": trend_summary,
        "market_sentiment_score": round(market_sentiment_score, 3),
        "nasdaq_data": nasdaq_data,
        "kospi_data": kospi_data,
        "fx_data": fx_data,
    }

    with open('data/daily_data.json', 'w', encoding='utf-8') as f:
        json.dump(final_data, f, ensure_ascii=False, indent=2)
    
    print("\nğŸš€ ëª¨ë“  ë°ì´í„°ê°€ 'data/daily_data.json'ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")